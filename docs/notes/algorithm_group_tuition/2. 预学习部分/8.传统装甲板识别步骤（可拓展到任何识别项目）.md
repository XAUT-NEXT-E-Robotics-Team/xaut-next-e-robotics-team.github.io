---
title: 传统装甲板识别步骤（可拓展到任何识别项目）
createTime: 2025/09/20 17:50:19
permalink: /docs/algorithm_group_tuition/6kvu9mai/
---

<!--
 *  _   _  _______   _______   _____  
 * | \ | ||  ___\ \ / /_   _| |  ___| 
 * |  \| || |__  \ V /  | |   | |__   
 * | . ` ||  __| /   \  | |   |  __|  
 * | |\  || |___/ /^\ \ | |   | |___  
 * \_| \_/\____/\/   \/ \_/   \____/  
 * 
 * @Author: ziyu (Chen Zhaoyu)
 * @Date: 2025-09-20 17:50:19
 * @LastEditors: ziyu (Chen Zhaoyu)
 * @LastEditTime: 2025-09-20 17:50:37
 * @Description: 
 * Copyright (c) 2025 by XAUT NEXT-E/ziyu, All Rights Reserved. 
-->

::: tip 
在预学习阶段，实现一个自瞄识别器是极为困难的。

不过，了解如何实现一个识别器对你的视觉识别算法学习和后续的项目开发（无论是什么项目）都有帮助

因此，这部分内容将会在后续作为答辩考核内容，请各位务必阅读学习。
:::

::: tip
由于这位菜狗（指笔者）文笔和技术都很垃圾，因此部分内容摘自[视觉圣经](https://github.com/NeoZng/vision_tutorial/tree/v1.0.0)，有兴趣的可以直接去看原文（也推荐这么做）
:::


## ==一、 整车跟踪自瞄算法的起源==
### 1. 自瞄算法
&emsp;&emsp;在RM赛场中，开挂是被允许的。自动瞄准系统（自瞄）利用相机的识别和定位算法以达到指导云台瞄准目标，提高命中率的作用。为此，RM视觉组由此诞生。
&emsp;&emsp;传统的自瞄思路非常简单：识别装甲板，确定装甲板位置，计算云台目标位置，开火。在自瞄刚出现的几个赛季内，这种传统算法的效果非常不错。然而好景不长，许多学校为此做出了改进。在当今的RM赛场上，绝大多数学校会选择使用两对装甲板不同高度的机械设计和底盘高速自旋（小陀螺）的运动方式以达到移动装甲板，以防止命中的目的。对于传统自瞄，由于单个目标的局限性，即使建立高阶状态空间方程（一种建模方法，不重要）并使用卡尔曼滤波（一种算法，不重要，认为它很牛逼就行）进行观测，仍较难获得该装甲板的运动状态，特别是当该装甲板即将旋转90度（即将被隐藏不可击打）的时装甲板预测将会飘出车外，严重影响精度。

![](https://free.picui.cn/free/2025/09/20/68ce7f10b04eb.png) 机器人发生旋转的时候，由于装甲板转到另一侧导致预测失效超出车体；图源华广机器人RMUA技术报告。

::: note 你在TM在说什么，可以讲人话吗？

试图去理解就行，也可以STFW辅助理解。

本文大部分内容摘自《2025完整形态技术文档》，是参加对抗赛必须要提交的内容。本文已进行简化处理。

你们以后也会写类似这样的东西，加油！

:::

### 2. 主流的自瞄算法
&emsp;&emsp;由于以上需求和原因，几乎所有RoboMaster有能力开发自瞄的战队均将自瞄研发的重心从传统单装甲板自瞄转移到研发可反小陀螺的自瞄系统中来。当前存在的反陀螺自瞄方案主要有两种，第一种是基于扩展卡尔曼滤波的整车观测器，即通过观测敌车装甲板以达到观测敌车陀螺半径，平面移动的3ODF自由度位姿数据和速度数据即装甲板高度等状态信息，并通过状态转移函数进行预测。第二种思路是不进行整车建模，只通过装甲板的出现与隐藏大致判断云台应预先摆放的位置，并当预期装甲板出现时，进行跟踪预测。相比于后者，经过我们和其他学校的实验验证，整车建模方案有效性和鲁棒性均较高。

&emsp;&emsp;现有主流自瞄开源有上海交通大学云台交龙战队再2023年青年工程师大会上开源的自瞄“方瞄” 和华南师范大学PIONEER战队chenjunnn在2022年开源自瞄系统“君瞄” 。由于自瞄系统开发的难度较大，需要较好的数理知识，逻辑能力，编程能力。鉴于我对2024赛季视觉组人数较少，能力和经验均不足，我队2024赛季选择基于华南师范大学PIONEER战队chenjunnn开源进行二次开发，增加了选板和串口等重要模块，以达到上场使用的目的。但由于”君瞄“自身缺陷和对开源算法的理解薄弱，2024赛季最优静止旋转靶自瞄命中率仅仅达到75%，而平均命中率不及60%，场上命中率低于50%，并不理想。鉴于上赛季的问题，2025赛季视觉组对自瞄系统的相关代码资料进行调查研究，从原理方面推导各类公式，对逻辑进行修正，采用了一套不同于开源自喵的方案，从0开始编写整套自瞄跟踪器代码，从而实现了代码的自有化。经过先前的测试，在控制方面优秀的前提下，静止小陀螺自瞄最高命中率达到了96，平均命中率也超过60%，超越24赛季。

### 3. NEXT E 的自瞄算法流程
![自瞄算法流程](https://free.picui.cn/free/2025/09/20/68ce79d76ccec.png)
1. 图像读取模块：使用海康相机SDK进行图像读取
2. 装甲板识别模块：使用传统方案进行装甲板识别，并使用LeNet5进行数字识别
3. PNP解算定位模块：使用PNP算法对装甲板位姿进行解算，并根据云台当前位姿将装甲板位姿转换到陀螺仪坐标系。
4. 扩展卡尔曼观测器模块：使用EKF对所得到的装甲板进行观测，以得到敌方机器人的底盘运动3ODF位姿与速度，装甲板高度和装甲板据机器人中心距离。
5. 整车模型预测模块：使用设计好一阶整车状态空间方程，并利用算法延迟和弹丸飞行时间对敌车运动情况进行预测。
6. 目标选择模块：利用装甲板类型，目标距离操作手准心的最近yaw等作为依据进行目标选择。
7. 水平方向空气阻力模型弹道补偿模块：为了补偿由重力和空气阻力造成的弹道下坠，建立的仅水平方向空气阻力模型进行弹道补偿。
8. 火控模块：基于电控当前值是否近似等于目标值为依据进行火控。
 
## ==二、传统装甲板识别步骤==
识别器大致工作流程如下：

![传统装甲板识别流程](https://free.picui.cn/free/2025/09/20/68ce798790339.png)

::: tip 没听过的算法名称？
请自行 STFW 或 RTFM
:::
在识别器中，二值化部分、颜色判断部分、形态判断部分和数字提取部分属于传统视觉的范畴。

### 1. 预处理部分

&emsp;&emsp;首先，我们选用合适的阈值对照片进行二值化，考虑到RM赛场对光线（光泽度）和颜色管理严格，因此，目标机器人上的杂光较少。实验证明，在 0 - 1500 lux的广泛照度下，在阈值合适的情况下，仅需选择合适的曝光均能够通过二值化稳定对灯条进行分离。

```C++
cv::Mat Detector::preprocessImage(const cv::Mat & rgb_img)//传入rgb类型图片
{
    cv::Mat gray_img;
    cv::cvtColor(rgb_img, gray_img, cv::COLOR_RGB2GRAY);//灰度

    cv::Mat binary_img;
    cv::threshold(gray_img, binary_img, binary_thres, 255, cv::THRESH_BINARY);//二值化

    return binary_img;
}
```

（1）转成灰度图片***cvtColor();***

（2）进行二值化处理处理成黑白图片***threshold();***
### 2. 颜色识别和查找灯条部分

&emsp;&emsp;接下来是颜色识别，通过相机照片可以看到，一块正常亮度的装甲板，灯条中心较量的部分呈现的颜色往往趋近于白色。因此仅通过提取颜色（蓝色或红色）通道进行阈值化识别将会获得一个中空的灯条，并不适合用来进行颜色判断。因此我们采用直接框选上一步识别的二值化灯条部分为ROI并逐个统计其中颜色像素和红色像素的值，进行比对，判断该灯条为蓝色或是红色，如果灯条不是预期敌方颜色，则不进行识别。

&emsp;&emsp;颜色识别后将进行灯条的形态判断，通过利用灯条的长宽比例判断是否是灯条还是别的东西，并利用最小矩形拟合结合判断得到灯条的最高点和最低点，进入下一步。

![](https://free.picui.cn/free/2025/09/20/68ce8293e544e.png)

（1）使用 ***cv::findContours()*** 查找轮廓

（2）对每个轮廓计算最小外接矩形 ***cv::minAreaRect()***

```C++
RotatedRect cv::minAreaRect(InputArray contours) 

// 参数：contour 是点的集合，如轮廓

// 返回值：RotatedRect类型，即带角度的旋转矩形框,其值形如(center(x,y), (width, height), angle)
```

- 随后通过调用返回值对象的 points 方法来获取这个矩形的四个顶点
- 对四个顶点的 y 坐标进行排序 std::sort() ,并且得到顶部和底部坐标，长度，宽度和角度（弧度制）


（3）进一步用上面得到的数据来筛选是否为装甲板上的灯条，筛选参数
``` C++
bool Detector::isLight(const Light & light)
{
    float ratio = light.width / light.length;           // 宽高比
    bool ratio_ok = l.min_ratio < ratio && ratio < l.max_ratio;
    bool angle_ok = light.tilt_angle < l.max_angle;     // 倾斜角度
    return ratio_ok && angle_ok;
}
```

（4）根据RGB通道值判断灯条颜色（红/蓝）

### 3. 灯条匹配

&emsp;&emsp;接下来，利用穷举的方法对灯条进行匹配，排除明显不符合要求的匹配结果（如两个灯条中间还有一个灯条的情况）并得到装甲板灯条的四个顶点位置。通过按照特定比例扩展灯条长度即可得到数字所在的范围，将数字提取并通过仿射变换和均值化，得到一张装甲板数字图片进行接下来的数字识别。

（1）双循环匹配所有灯条对

```C++
for (auto light_1 = lights.begin(); light_1 != lights.end(); light_1++) 
{
    for (auto light_2 = light_1 + 1; light_2 != lights.end(); light_2++) 
    {
        // 匹配逻辑
    }
}
```

（2）检查颜色是否符合要求，用之前采集的灯条红蓝信息

```C++
if (light_1->color != detect_color || light_2->color != detect_color) continue;
```

（3）排除包含其他灯条的配对

（4）根据几何特征（装甲板匹配条件）判断是否为有效装甲板

```C++
ArmorType Detector::isArmor(const Light & light_1, const Light & light_2)
{
    float light_length_ratio = ...;                    // 灯条长度比
    float center_distance = ...;                       // 中心距离
    float angle = ...;                                 // 连接线角度

    // 验证条件
    bool light_ratio_ok = light_length_ratio > a.min_light_ratio;
    bool center_distance_ok = ...;
    bool angle_ok = angle < a.max_angle;

    return light_ratio_ok && center_distance_ok && angle_ok;
}
```

### 4. 数字识别

&emsp;&emsp;为唯一确定装甲板，使用几何条件是不够的。然而在RM装甲板上有一个唯一的条件可以被我们使用，就是我们的装甲板数字。因此，我们可以利用各类算法开始可以识别装甲板数字的程序。我们选用的是LeNet5算法。由于数字识别部分牵扯到机器学习和一些比较深入的传统cv内容比如仿射变换等，因此关于数字识别的算法细节和实现方式此处暂不引出，进入正式学习时将有学长学习详细培训。

## 三、 各种识别算法的比较

&emsp;&emsp;当今的视觉识别一般分为两类，机器学习和传统视觉，以及二者相融合的方法

:::table align="center"
| 比较项目 | 说明 | 传统视觉 | 机器学习 | 融合算法 |
|-----|-----|-----|-----|-----|
| 应用范围 |  | 窄（只能适用于较为单一的工况） | 宽 | 宽 |
| 鲁棒性 | 耐操性 | 一般较差（易受光线影响） | 一般较强（取决于算法选择和数据集质量） | 不好说（取决于调参情况，上限及高下限及低） |
| 速度 |  | 快 | 慢 | 适中（一般不会使用非常复杂的网络） |
| 跨平台 |  | 好 | 差（RM工况下一般只能使用Jetson或者i5或以上小电脑 | 还行，取决于网络（融合算法一般不适用复杂的网络） |
| 精确性 | 比如交点是否精确 | 好 | 很差（一般取决于数据集，而且神经网络为概率模型，精度不可控） | 好（可以兼顾二者优势） |
| 易实现性 | 好写？ | 好（传统视觉算法好理解，有库） | 不好说（看你自研还是用常用模型比如yolo）|  还行（可能需要你对简单网络的理解） |
| 易用性 | 好调？ | 好（算法好理解） | 难（炼丹） | 看你设计 |
| 评价 | 胡言乱语 | 只在特定工况适用（工业常用）一般需要补光或是可以实时调参的场景 | 懒人专用 or 大佬专用 | 要不成神，要不又调参又炼丹 |
:::

自瞄算法选用融合算法的策略。

::: tip 小巧思（作业）
你想识别什么呢，构思一个你自己的识别器。
:::